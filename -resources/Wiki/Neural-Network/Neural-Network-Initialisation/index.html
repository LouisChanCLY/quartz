<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="Metadata Association: Domain: [[Domain Root - Data Science]]
Neural Network Initialisation [[Neural Network]] initialisation is a process of setting initial weights to all the neurons of a neural network."><title>ðŸª´ Quartz 3.2</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://louischancly.github.io/quartz//icon.png><link href=https://louischancly.github.io/quartz/styles.b54f56dc7e7fdf6be9a4427dc6b0509b.min.css rel=stylesheet><link href=https://louischancly.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://louischancly.github.io/quartz/js/darkmode.7230926211c2934ae3b42be19ab59403.min.js></script>
<script src=https://louischancly.github.io/quartz/js/util.59b5eb848f398a9b2cf864a089780673.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://louischancly.github.io/quartz/js/popover.37b1455b8f0603154072b9467132c659.min.js></script>
<script src=https://louischancly.github.io/quartz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://louischancly.github.io/quartz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://louischancly.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://louischancly.github.io/quartz/",fetchData=Promise.all([fetch("https://louischancly.github.io/quartz/indices/linkIndex.2a4fb4c45e5249a9abfb8a2fa92826b3.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://louischancly.github.io/quartz/indices/contentIndex.ebff0e7adc280cb92423b398e4a35f35.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const n=new URL(BASE_URL),s=n.pathname,o=window.location.pathname,i=s==o;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts();const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=i&&!1;drawGraph("https://louischancly.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2}),initPopover("https://louischancly.github.io/quartz",!0,!0)},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/louischancly.github.io\/quartz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script defer src=https://louischancly.github.io/quartz/js/semantic-search.2aaf0645c253035a19cffaf67ea7b2d8.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://louischancly.github.io/quartz/>ðŸª´ Quartz 3.2</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Jul 19, 2022
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/%e2%8c%96%20resources/Wiki/Neural%20Network/Neural%20Network%20Initialisation.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#metadata>Metadata</a></li><li><a href=#neural-network-initialisation>Neural Network Initialisation</a></li><li><a href=#rule-of-thumb-for-weight-initialisation>Rule of Thumb for Weight Initialisation</a></li><li><a href=#zero-initialisation---symmetry-problem>[[Zero Initialisation]] - [[Symmetry Problem]]</a></li><li><a href=#too-small-initialisation---vanishing-gradient>Too-small Initialisation - [[Vanishing Gradient]]</a></li><li><a href=#too-large-initialisation--exploding-gradient>Too-large Initialisation â€” [[Exploding Gradient]]</a></li></ol></nav></details></aside><a href=#metadata><h2 id=metadata><span class=hanchor arialabel=Anchor># </span>Metadata</h2></a><p>Association:
Domain: <a class="internal-link broken">Domain Root - Data Science</a></p><a href=#neural-network-initialisation><h2 id=neural-network-initialisation><span class=hanchor arialabel=Anchor># </span>Neural Network Initialisation</h2></a><p><a class="internal-link broken">Neural Network</a> initialisation is a process of setting initial weights to all the neurons of a neural network. These weights will be used for computing the very first batch of scores before being updated by <a class="internal-link broken">Backward Propagation</a> after lost/cost has been computed.</p><p>Initialising weights for <a class="internal-link broken">Neural Network</a> are usually as simple as a oneliner:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># one-liner weight initialisation for tensorflow</span>
</span></span><span class=line><span class=cl><span class=n>tf</span><span class=o>.</span><span class=n>keras</span><span class=o>.</span><span class=n>initializers</span><span class=o>.</span><span class=n>RandomUniform</span><span class=p>(</span><span class=n>minval</span><span class=o>=-</span><span class=mf>0.05</span><span class=p>,</span> <span class=n>maxval</span><span class=o>=</span><span class=mf>0.05</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=kc>None</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><a href=#rule-of-thumb-for-weight-initialisation><h2 id=rule-of-thumb-for-weight-initialisation><span class=hanchor arialabel=Anchor># </span>Rule of Thumb for Weight Initialisation</h2></a><p>As deep learning models get more and more complex, the problem of <a class="internal-link broken">Vanishing Gradient</a> and <a class="internal-link broken">Exploding Gradient</a> will only be more pronounced.Â <strong>A general rule of thumb for initialising weights is to (1) make it random, (2) have a stable variance for each layer, and (3) potentially make the distribution symmetric.</strong>Â Based on that, there has been a lot of excellent work dedicated to weight initialisation aiming at better andÂ <strong>faster convergence with reduced gradient problems</strong>. Here are two of the more common initialisation methods that you may find useful for building your own deep learning models:</p><ol><li><ul><li><strong><a class="internal-link broken">He Initialisation</a></strong>Â 
<a href=https://arxiv.org/abs/1502.01852 rel=noopener><strong>(He et al. 2015)</strong></a><strong>:</strong>Â This is more suited for initialising weights for <a class="internal-link broken">ReLu</a> and <a class="internal-link broken">Leaky ReLu</a> <a class="internal-link broken">Activation Function</a>.
$$W^{[l]} = N(0, \sqrt{\frac{2}{size^{[l-1]}}})$$</li></ul></li><li><ul><li><strong><a class="internal-link broken">Xavier Initialisation</a></strong>Â 
<a href=http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf rel=noopener><strong>(Glorot et al. 2010)</strong></a><strong>:</strong>Â This is more suited for initialising weights for <a class="internal-link broken">Hyperbolic Tangent</a> <a class="internal-link broken">Activation Function</a>
$$W^{[l]} = N(0, \sqrt{\frac{1}{size^{[l-1]}}})$$</li></ul></li></ol><p>That being said, there are some common pitfalls when it comes to <a class="internal-link broken">Neural Network</a> initialisation.</p><a href=#zero-initialisation---symmetry-problem><h2 id=zero-initialisation---symmetry-problem><span class=hanchor arialabel=Anchor># </span><a class="internal-link broken">Zero Initialisation</a> - <a class="internal-link broken">Symmetry Problem</a></h2></a><ul><li><strong>When does it happen:</strong> All initialised weights of a <a class="internal-link broken">Neural Network</a> is zero.</li><li><strong>Result:</strong> The <a class="internal-link broken">Neural Network</a> will become a <strong><a class="internal-link broken">Linear Model</a></strong></li></ul><p>One of the strengths of <a class="internal-link broken">Neural Network</a> are their <a class="internal-link broken">Non-linearity</a>. By setting all the weights to zero, all the <a class="internal-link broken">Partial Derivatives</a> in <a class="internal-link broken">Backward Propagation</a> will be the same for every neuron. While this won&rsquo;t break the algorithm, this will stall learning progress as all neurons in the same layer will be learning the same parameter.</p><a href=#too-small-initialisation---vanishing-gradient><h2 id=too-small-initialisation---vanishing-gradient><span class=hanchor arialabel=Anchor># </span>Too-small Initialisation - <a class="internal-link broken">Vanishing Gradient</a></h2></a><ul><li><strong>Why does it happen:</strong>Â Initialised weights of a neural network are too small</li><li><strong>Result:</strong>Â Premature convergence</li><li><strong>Symptoms:</strong>Â Model performance improves very slowly during training. The training process is also likely to stop very early.</li></ul><p>If the initial weights of the neurons are too small relative to the inputs, theÂ <strong>gradient of the hidden layers will diminish exponentially as we propagate backward</strong>. Or you may also say, vanishing through the layers. To better understand that, letâ€™s assume that we have a three-layer fully connected neural network below where every layer has a sigmoid activation function with zero bias:</p><p><img src="/quartz/Pasted image 20220719212521.png" width=auto></p><p>If we quickly recall how a <a class="internal-link broken">Sigmoid</a> function and its derivates look like, we can see that the maximum value of the derivative (i.e. gradient) of the sigmoid function is at 0.25 when $x=0$. For reference, derivative of a <a class="internal-link broken">Sigmoid</a> function is $\sigma(x)(1 - \sigma(x))$.</p><p>When training a model using <a class="internal-link broken">Gradient Descent</a>, we will be updating the weights of the entire neural network using <a class="internal-link broken">Backward Propagation</a> by taking partial derivatives of the loss values with respect to the weights of each layer. To take the partial derivates for the network we have above, we need to first know about the mathematical expression of it. Assuming</p><ul><li>$a$Â stands for the output vector of activation function,</li><li>$sigma$Â stands for the sigmoid function (i.e. our activation function),</li><li>$z$ stands for the output of the neurons of a layer, and</li><li>$W$Â stands for the vector of weights of each layer</li></ul><p>the output of the neural network above can be represented as follows:</p><p>$$\hat{y} = a^{[3]} = \sigma z^{[3]} = \sigma W^{[3]} a^{[2]} = &mldr; = \sigma W^{[3]} \sigma W^{[2]} \sigma W^{[1]} x$$</p><p>Hence, the partial derivative for updating $W^{[1]}$, weights in the first layer, will be as follows:</p><p>$$\frac{\partial Loss}{\partial W^{[1]}} = \frac{\partial Loss}{\partial a^{[3]}}\frac{\partial a^{[3]}}{\partial z^{[3]}}\frac{\partial z^{[3]}}{\partial a^{[2]}}\frac{\partial a^{[2]}}{\partial z^{[2]}}\frac{\partial z^{[2]}}{\partial a^{[1]}}\frac{\partial a^{[1]}}{\partial z^{[1]}}\frac{\partial z^{[1]}}{\partial W^{[1]}}$$</p><p>#todo ==Find more intuitive explanation for this please!==</p><p><strong>Ways to alleviate vanishing gradient can be:</strong></p><ul><li>LSTM can solve the vanishing gradient problem with its gates</li><li>Use activation function like ReLu or leaky ReLu which are both less prone to vanishing gradient</li><li>Reduce the number of layers</li><li>Randomly initialize weights at a sufficiently large expected value</li></ul><a href=#too-large-initialisation--exploding-gradient><h2 id=too-large-initialisation--exploding-gradient><span class=hanchor arialabel=Anchor># </span>Too-large Initialisation â€” <a class="internal-link broken">Exploding Gradient</a></h2></a><ul><li><strong>When does it happen:</strong>Â Initialised weights of a neural network are too large</li><li><strong>Result:</strong>Â Loss value will oscillate around minima but unable to converge</li><li><strong>Symptoms:</strong>Â The model will have large changes in its loss value on each update due to its instability. Loss values may also reach <code>NaN</code>.</li><li></li></ul><p>#todo ==Find more intuitive explanation for this please!==</p><p><strong>Ways to avoid exploding gradient can be:</strong></p><ul><li><a class="internal-link broken">Gradient Clipping</a> which limits the size of the gradient</li><li>Use activation function like <a class="internal-link broken">ReLu</a> or <a class="internal-link broken">Leaky ReLu</a> which are both less prone to exploding gradient</li><li>Randomly initialize weights with sufficiently small expected values</li><li>Reduce the number of layers</li></ul></article><hr><div class=page-end><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li>No backlinks found</li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://louischancly.github.io/quartz/js/graph.afdb02e537635f9a611b53a988e5645b.js></script></div></div><div id=contact_buttons><footer><p>Made by Jacky Zhao using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2022</p><ul><li><a href=https://louischancly.github.io/quartz/>Home</a></li><li><a href=https://twitter.com/_jzhao>Twitter</a></li><li><a href=https://github.com/jackyzha0>Github</a></li></ul></footer></div></div></body></html>